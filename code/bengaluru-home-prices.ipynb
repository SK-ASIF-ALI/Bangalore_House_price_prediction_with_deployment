{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"importing necessary liabraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Now we are going to read the data using pandas funciton","metadata":{}},{"cell_type":"code","source":"raw_data=pd.read_csv('../input/bengaluru-house-price-data/Bengaluru_House_Data.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"availability is not important in this dataset.we will drop it","metadata":{}},{"cell_type":"code","source":"data=raw_data.drop(['area_type','society'],axis='columns')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Search for the Nan values and their counts","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can simply delete those na values because our dataset is quite large","metadata":{}},{"cell_type":"code","source":"data.dropna(inplace=True)\ndata.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can see that there are no nan values.so we can continue the operation","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the dataset in the size column we can see that there are many units like bedroom or bhk etc","metadata":{}},{"cell_type":"code","source":"data['size'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as we can see that bhk and bedrooms are same so we will create a new column named as BHK and put the values from the size columns","metadata":{}},{"cell_type":"code","source":"data['BHK']=data['size'].apply(lambda x:int(x.split(' ')[0]))\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so we don't need the size column. so we will drop this columns","metadata":{}},{"cell_type":"code","source":"data.drop(['size','availability'],axis='columns',inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['BHK'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.total_sqft.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In our dataset total sqft contains some range values like 1133-1384. in this case we'll take their average.","metadata":{}},{"cell_type":"markdown","source":"We've made a function to convert the range values to float average value","metadata":{}},{"cell_type":"code","source":"def is_total(x):\n    try:\n        \n        token=x.split('-')\n        if(len(token)==2):\n            return (float(token[0])+float(token[1]))/2\n        else:\n            return float(x)\n    except:\n        return False\n        \n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1=data.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1['total_sqft']=data1['total_sqft'].apply(is_total)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.loc[30]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Price for square feet","metadata":{}},{"cell_type":"code","source":"data1['total_sqft']=data1['total_sqft'].astype('float64')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1['price_per_sqft']=data1['price']*100000/data1['total_sqft']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In our dataset or any real estate bussiness location is a very important feature. we can't just remove it.First we need to check how many unique location we do have","metadata":{}},{"cell_type":"code","source":"len(data1.location.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now group by and count them on using aggregate function","metadata":{}},{"cell_type":"code","source":"data1.location=data1.location.apply(lambda x:x.strip())\nlocation_stat=data.groupby('location')['location'].agg('count').sort_values(ascending=False)\nlocation_stat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"what we gonna we gonna call location_stats<=10 as other place","metadata":{}},{"cell_type":"code","source":"len(location_stat[location_stat<=10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1028 location will be known as other location\n","metadata":{}},{"cell_type":"code","source":"location_less10=location_stat[location_stat<=10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.location=data.location.apply(lambda x:'others' if x in location_less10 else x)\ndata1.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data1.location.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.head(500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outlier removal","metadata":{}},{"cell_type":"markdown","source":"now we need some domain level knowledge.like for 12 rooms total sqft is 500 is not acceptable.so i ask my project manager what is the typical value of sqft per bedroom. so we will use a threshold that if per room bhk < 300 then. so the threshold for each room is 300 sqft","metadata":{"trusted":true}},{"cell_type":"code","source":"data1[data1.total_sqft/data1.BHK < 300].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Look at the above data.3 rooms = 500 sqft is not possbile. so we need to remove them","metadata":{}},{"cell_type":"code","source":"data2=data1[~(data1.total_sqft/data1.BHK < 300)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2.total_sqft.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standard Deviation. The Standard Deviation is a measure of how spread out numbers are","metadata":{}},{"cell_type":"markdown","source":"we need to find out the mean and stadard deviation locationwise and remove price_persqft.filter out any data point beyond standard deviation","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_out(df):\n    df_out=pd.DataFrame()\n    for key,subdf in df.groupby('location'):\n        print(key)\n        m=np.mean(subdf.price_per_sqft)\n        st=np.std(subdf.price_per_sqft)\n        reduce=subdf[(subdf.price_per_sqft >(m-st)) & (subdf.price_per_sqft <= (m+st))]\n        df_out=pd.concat([df_out,reduce],ignore_index=True)\n    return df_out\n\n\ndata3=remove_out(data2)\ndata3.shape\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the dataset we have found that the price of 2bhk is greater than 3bhk of same price per area.This is an error so we need to remove them.Let's see it by using the plots","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport matplotlib as matplotlib\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_s(df,location):\n    bhk2=df[((df.location==location) & (df.BHK==2))]\n    bhk3=df[((df.location==location) & (df.BHK==3))]\n    matplotlib.rcParams['figure.figsize']=(15,10)\n    plt.scatter(bhk2.total_sqft,bhk2.price,color='blue',label='2 BHK',s=50)\n    plt.scatter(bhk3.total_sqft,bhk3.price,color='red',label='3 BHK',s=50,marker='*')\n    plt.xlabel('Total Square foot area')\n\n    plt.ylabel('Price')\n    plt.title('Location')\n    plt.legend()\n    \n    \nplot_s(data3,'Hebbal')\n    \n    \n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see that the price of 2 bhk flt is greater than 3bhk flat of same area.so we need to remove the outliers inorder to have a clean dataset\n\n{\n\n'1':{\n    'mean':4000,\n    'std':2000,\n    'count':34\n    },\n'2':{\n    'mean':4300,\n    'std':2300,\n    'count':22\n    },\n}\n\nnow we can remove those 2 bhk flats whose price_per_sqft is less than then the mean of price_per_sqft flat;\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_bhk_outlier(df):\n    exclude_indices=np.array([])\n    for location,location_df in df.groupby('location'):\n        bhk_stats={}\n        for bhk,bhk_df in location_df.groupby('BHK'):\n            bhk_stats[bhk]={\n                'mean':np.mean(bhk_df.price_per_sqft),\n                'std':np.std(bhk_df.price_per_sqft),\n                'count':bhk_df.shape[0]\n            }\n        for bhk,bhk_df in location_df.groupby('BHK'):\n            stats=bhk_stats.get(bhk-1)\n            if stats and stats['count']>5:\n                exclude_indices=np.append(exclude_indices,bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)\n    return df.drop(exclude_indices,axis='index')\n\ndata4=remove_bhk_outlier(data3)\ndata4.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_s(data4,'Hebbal')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PLOT THE HISTOGRAM TO SEE THE DATA DISTRIBUTION","metadata":{}},{"cell_type":"code","source":"matplotlib.rcParams['figure.figsize']= (20,10)\nplt.hist(data4.price_per_sqft,rwidth=0.8)\nplt.xlabel('Price per square feet')\nplt.ylabel('count')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"majority of 0-10k rupees ","metadata":{}},{"cell_type":"code","source":"data4.bath.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data4[data4.bath>4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for a single room there should be max 2 bathrooms","metadata":{}},{"cell_type":"code","source":"matplotlib.rcParams['figure.figsize']= (20,10)\nplt.hist(data4.bath,rwidth=0.8)\nplt.xlabel('Bathroom')\nplt.ylabel('count')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data4[data4.bath<data4.BHK+2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5=data4[data4.bath<data4.BHK+2]\ndata5.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data5.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"price_per_sqft,size can be droppped.These are not useful right on","metadata":{}},{"cell_type":"code","source":"data6=data5.drop(['price_per_sqft','balcony'],axis='columns')\ndata6.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MODEL BUILDING\n","metadata":{}},{"cell_type":"code","source":"data6.location.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we need to use one hot encoding.with get dummies","metadata":{}},{"cell_type":"code","source":"dum=pd.get_dummies(data6.location)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data7=pd.concat([data6,dum.drop('others',axis='columns')],axis='columns')\ndata7.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data7.drop('location',axis='columns',inplace=True)\ndata7.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we've sucessfully cleaned our data.we need to split our data into input and target","metadata":{}},{"cell_type":"code","source":"input=data7.drop('price',axis='columns')\ntarget=data7.price","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we need to split the data into training and testing using train_test_split for linear regression model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(input,target,test_size=0.2,random_state=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we need to import Linear regression from sklearn to  implement it","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()\nlr.fit(x_train,y_train)\nlr.score(x_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll try to solve it by using cross validation method","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\n\ncv=ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\ncross_val_score(LinearRegression(),input,target,cv=cv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are various regression technique not only linear regression.we can use lasso,decision tree and various using Gidsearchcv method","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef find_best_model_using_gridsearchcv(X,y):\n    algos = {\n        'linear_regression' : {\n            'model': LinearRegression(),\n            'params': {\n                'normalize': [True, False]\n            }\n        },\n        'lasso': {\n            'model': Lasso(),\n            'params': {\n                'alpha': [1,2],\n                'selection': ['random', 'cyclic']\n            }\n        },\n        'decision_tree': {\n            'model': DecisionTreeRegressor(),\n            'params': {\n                'criterion' : ['mse','friedman_mse'],\n                'splitter': ['best','random']\n            }\n        }\n    }\n    scores = []\n    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n    for algo_name, config in algos.items():\n        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)\n        gs.fit(X,y)\n        scores.append({\n            'model': algo_name,\n            'best_score': gs.best_score_,\n            'best_params': gs.best_params_\n        })\n\n    return pd.DataFrame(scores,columns=['model','best_score','best_params'])\n\nfind_best_model_using_gridsearchcv(input,target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_price(location,sqft,bath,bhk):    \n    loc_index = np.where(X.columns==location)[0][0]\n\n    x = np.zeros(len(X.columns))\n    x[0] = sqft\n    x[1] = bath\n    x[2] = bhk\n    if loc_index >= 0:\n        x[loc_index] = 1\n\n    return lr.predict([x])[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_price('1st Phase JP Nagar',1000, 3, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #export the model\nimport pickle\nwith open('./bangalore_house_price_prediction.pickle','wb') as f:\n    pickle.dump(lr,f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\ncolumns={\n    'data_columns':[col.lower() for col in X.columns]\n}\nwith open(\"./Columns.json\",\"w\") as f:\n    f.write(json.dumps(columns))","metadata":{},"execution_count":null,"outputs":[]}]}